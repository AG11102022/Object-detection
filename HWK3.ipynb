{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd533bc2",
   "metadata": {},
   "source": [
    "## Downloading and Storing the test Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f579216",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_urls = [\n",
    "    \"https://www.youtube.com/watch?v=DhmZ6W1UAv4\",\n",
    "    \"https://www.youtube.com/watch?v=YrydHPwRelI\"\n",
    "]\n",
    "\n",
    "local_paths = [\n",
    "    \"C:/Users/algba/AI_Assignments/A3/video1.mp4\",\n",
    "    \"C:/Users/algba/AI_Assignments/A3/video2.mp4\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54225c80",
   "metadata": {},
   "source": [
    "### Function to download videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eab846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "def download_video(url, local_path):\n",
    "    ydl_opts = {\n",
    "        'outtmpl': local_path,\n",
    "        'format': 'mp4'\n",
    "    }\n",
    "    with YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([url])\n",
    "\n",
    "for url, path in zip(video_urls, local_paths):\n",
    "    download_video(url, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c782582",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in local_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Video downloaded successfully: {path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download video: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b640629a",
   "metadata": {},
   "source": [
    "### Split Videos into Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff5051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d293570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_video_to_frames(video_path, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:04d}.png\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames from {video_path}\")\n",
    "\n",
    "video_paths = [\n",
    "    \"C:/Users/algba/AI_Assignments/A3/video1.mp4\",\n",
    "    \"C:/Users/algba/AI_Assignments/A3/video2.mp4\"\n",
    "]\n",
    "\n",
    "for video_path in video_paths:\n",
    "    output_dir = os.path.splitext(video_path)[0] + \"_frames\"\n",
    "    split_video_to_frames(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d40432",
   "metadata": {},
   "source": [
    "### Preprocess Frames\n",
    "##### Video 2 had a lot of frames to preprocess and I ran out of memory to continue. So just focusing on Video 1 but code will take any other video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfa10620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(input_dir, output_file):\n",
    "    frame_files = [f for f in os.listdir(input_dir) if f.endswith('.png')]\n",
    "    frame_files.sort()\n",
    "\n",
    "    processed_frames = []\n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(input_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "\n",
    "        # Resize frame\n",
    "        resized_frame = cv2.resize(frame, (224, 224))\n",
    "\n",
    "        # Convert color from BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize the frame\n",
    "        normalized_frame = rgb_frame / 255.0\n",
    "\n",
    "        processed_frames.append(normalized_frame)\n",
    "\n",
    "    # Save processed frames to a .npy file\n",
    "    np.save(output_file, np.array(processed_frames))\n",
    "    print(f\"Saved processed frames to {output_file}\")\n",
    "\n",
    "for video_path in video_paths:\n",
    "    input_dir = os.path.splitext(video_path)[0] + \"_frames\"\n",
    "    output_file = os.path.splitext(video_path)[0] + \"_frames.npy\"\n",
    "    preprocess_frames(input_dir, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a1c6e",
   "metadata": {},
   "source": [
    "## Detect Drone in Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d43d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pre-trained YOLO model\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92128132",
   "metadata": {},
   "source": [
    "### Creating virtual environment (I was having issues with some modules that I had to update for other stuff) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f5f486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\algba\\AI_Assignments\\A3\n"
     ]
    }
   ],
   "source": [
    "cd C:/Users/algba/AI_Assignments/A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b11cc59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.9.13.final.0-64 in 2415ms\n",
      "  creator CPython3Windows(dest=C:\\Users\\algba\\AI_Assignments\\A3\\venv, clear=False, no_vcs_ignore=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=C:\\Users\\algba\\AppData\\Local\\pypa\\virtualenv)\n",
      "    added seed packages: pip==24.1, setuptools==70.1.0, wheel==0.43.0\n",
      "  activators BashActivator,BatchActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
     ]
    }
   ],
   "source": [
    "!virtualenv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "929623a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7dd6dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v7.0-338-gff063284 Python-3.9.13 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('./yolov5', 'yolov5s', source='local')  # Load model from local clone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12acceaa",
   "metadata": {},
   "source": [
    "### Detecting objects in frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "\n",
    "# Initialize YOLO model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True).to(device)\n",
    "\n",
    "def detect_objects_in_frames(input_file, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    frames = np.load(input_file)\n",
    "    transform = T.ToTensor()\n",
    "    total_frames = len(frames)\n",
    "    print(f\"Total frames to process: {total_frames}\")\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        print(f\"Processing frame {i+1}/{total_frames}\")\n",
    "        frame = (frame * 255).astype(np.uint8)\n",
    "        frame_pil = Image.fromarray(frame)\n",
    "        frame_tensor = transform(frame_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        # object detection\n",
    "        results = model(frame_tensor)\n",
    "\n",
    "        # Numpy array \n",
    "        results_np = results[0].detach().cpu().numpy()\n",
    "\n",
    "        # Checking for drone detections\n",
    "        detections_found = False\n",
    "        for det in results_np:\n",
    "            x1, y1, x2, y2, confidence, class_id = det[:6]\n",
    "            if confidence > 0.3 and int(class_id) == 0:  # Assuming class 0 is for drone\n",
    "                detections_found = True\n",
    "                detection_path = os.path.join(output_dir, f\"detection_{i:04d}.png\")\n",
    "                frame_with_detections = frame.copy()\n",
    "\n",
    "                cv2.rectangle(frame_with_detections, \n",
    "                              (int(x1), int(y1)), \n",
    "                              (int(x2), int(y2)), \n",
    "                              (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imwrite(detection_path, cv2.cvtColor(frame_with_detections, cv2.COLOR_RGB2BGR))\n",
    "                print(f\"Detected drone in frame {i+1} with confidence {confidence}, bbox: {int(x1), int(y1), int(x2), int(y2)}\")\n",
    "\n",
    "        if not detections_found:\n",
    "            print(f\"No detections in frame {i+1}\")\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "# Process the frames from the first video\n",
    "input_file = \"C:/Users/algba/AI_Assignments/A3/video1_frames.npy\"\n",
    "output_dir = \"C:/Users/algba/AI_Assignments/A3/video1_detections\"\n",
    "detect_objects_in_frames(input_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e370367",
   "metadata": {},
   "source": [
    "### kalman Fiter for Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1edaf3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\algba/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-7-10 Python-3.9.13 torch-2.2.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from filterpy.kalman import KalmanFilter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Initialize Kalman Filter\n",
    "kf = KalmanFilter(dim_x=4, dim_z=2)  # 2D state (x, y, dx, dy)\n",
    "\n",
    "# Initial coordinates based on detection\n",
    "initial_x = 100\n",
    "initial_y = 100\n",
    "\n",
    "kf.x = np.array([initial_x, initial_y, 0., 0.])  # initial state: position and velocity\n",
    "kf.F = np.array([[1, 0, 1, 0],\n",
    "                 [0, 1, 0, 1],\n",
    "                 [0, 0, 1, 0],\n",
    "                 [0, 0, 0, 1]])  # state transition matrix\n",
    "\n",
    "# Measurement matrix\n",
    "kf.H = np.array([[1, 0, 0, 0],\n",
    "                 [0, 1, 0, 0]])\n",
    "\n",
    "# Measurement noise covariance\n",
    "kf.R = np.eye(2) * 0.5\n",
    "\n",
    "# Noise covariance\n",
    "kf.Q = np.eye(4) * 0.1\n",
    "\n",
    "# Loading YOLO model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Loading video\n",
    "video_path = \"C:/Users/algba/AI_Assignments/A3/video1.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error opening video file at {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Output video settings\n",
    "output_path = \"C:/Users/algba/AI_Assignments/A3/tracked_video1.mp4\"\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "if not out.isOpened():\n",
    "    print(f\"Error opening output video file at {output_path}\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Performing detection of drone position using YOLO\n",
    "    results = model(frame)\n",
    "    detections = results.xyxy[0].cpu().numpy()\n",
    "    \n",
    "    # Finding the largest detection (drone)\n",
    "    if len(detections) > 0:\n",
    "        detection = max(detections, key=lambda x: x[4])  # x[4] is the confidence score\n",
    "        x1, y1, x2, y2, confidence, class_id = detection\n",
    "        detected_x = (x1 + x2) / 2\n",
    "        detected_y = (y1 + y2) / 2\n",
    "        \n",
    "        # Updating Kalman filter with detected position\n",
    "        kf.predict()\n",
    "        kf.update([detected_x, detected_y])\n",
    "        \n",
    "        # Estimating state from Kalman filter\n",
    "        estimated_x = int(kf.x[0])\n",
    "        estimated_y = int(kf.x[1])\n",
    "        estimated_dx = int(kf.x[2])\n",
    "        estimated_dy = int(kf.x[3])\n",
    "        \n",
    "        # Calculating predicted next position based on estimated velocity\n",
    "        predicted_x = estimated_x + estimated_dx\n",
    "        predicted_y = estimated_y + estimated_dy\n",
    "        \n",
    "        # Updating bounding box coordinates based on predicted position\n",
    "        bbox_x1 = predicted_x - 50\n",
    "        bbox_y1 = predicted_y - 50\n",
    "        bbox_x2 = predicted_x + 50\n",
    "        bbox_y2 = predicted_y + 50\n",
    "        \n",
    "        # Drawing green rectangle based on updated bounding box coordinates\n",
    "        cv2.rectangle(frame, (bbox_x1, bbox_y1), (bbox_x2, bbox_y2), (0, 255, 0), 2)\n",
    "        \n",
    "    # Writing the frame into the output video\n",
    "    out.write(frame)\n",
    "    \n",
    "    cv2.imshow('Tracked Video Frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Model summary\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
